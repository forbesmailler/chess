{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class ChessCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChessCNN, self).__init__()\n",
    "        # The board state is 768 features reshaped into (12, 8, 8)\n",
    "        self.conv1 = nn.Conv2d(in_channels=12, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        # After two conv layers, the board output is 64 channels of size 8x8 = 4096 features.\n",
    "        # We then combine these with the extra 12 features.\n",
    "        self.fc1 = nn.Linear(4096 + 12, 512)\n",
    "        self.fc2 = nn.Linear(512, 3)  # 3 classes: win, draw, loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 780)\n",
    "        board = x[:, :768]  # First 768 features for board state\n",
    "        extras = x[:, 768:] # Next 12 features (en passant + castling)\n",
    "        \n",
    "        # Reshape board to (batch, 12, 8, 8)\n",
    "        board = board.view(-1, 12, 8, 8)\n",
    "        x_board = F.relu(self.conv1(board))\n",
    "        x_board = F.relu(self.conv2(x_board))\n",
    "        \n",
    "        # Flatten the convolutional features\n",
    "        x_board = x_board.view(x_board.size(0), -1)\n",
    "        \n",
    "        # Concatenate with the extra features\n",
    "        x_combined = torch.cat([x_board, extras], dim=1)\n",
    "        x_combined = F.relu(self.fc1(x_combined))\n",
    "        logits = self.fc2(x_combined)\n",
    "        return logits\n",
    "\n",
    "# Example training code using dummy data for demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Create random dummy data: 1000 samples with 780 features each\n",
    "    num_samples = 1000\n",
    "    X = torch.randn(num_samples, 780).to(device)\n",
    "    y = torch.randint(0, 3, (num_samples,)).to(device)  # Labels: 0 (loss), 1 (draw), 2 (win)\n",
    "    \n",
    "    # Create DataLoader for batching\n",
    "    dataset = TensorDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    model = ChessCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for batch_x, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * batch_x.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / num_samples\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
