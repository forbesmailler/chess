{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb321fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing train: 100%|██████████| 1000000/1000000 [01:15<00:00, 13185.79it/s]\n",
      "Featurizing val: 100%|██████████| 100000/100000 [00:07<00:00, 13311.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting logistic regression...\n",
      "Logistic Regression - Validation accuracy: 0.6026"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['chess_lr.joblib']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import chess\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "def extract_features(fen: str) -> np.ndarray:\n",
    "    board = chess.Board(fen)\n",
    "    arr = np.zeros(12 * 64, dtype=np.float32)\n",
    "    for sq, piece in board.piece_map().items():\n",
    "        ch = (piece.piece_type - 1) + (0 if piece.color == chess.WHITE else 6)\n",
    "        arr[ch * 64 + sq] = 1.0\n",
    "    n_pieces = len(board.piece_map())\n",
    "    factor1 = (n_pieces - 2) / 30\n",
    "    factor2 = (32 - n_pieces) / 30\n",
    "    return np.concatenate([arr * factor1, arr * factor2], axis=0)\n",
    "\n",
    "def process_dataset(df: pd.DataFrame, size: int, desc: str):\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True).iloc[:size]\n",
    "    X = np.zeros((len(df), 2 * 12 * 64), dtype=np.float32)\n",
    "    y = df['value'].values.astype(np.float32)\n",
    "    for i, fen in tqdm(enumerate(df['FEN']), total=len(df), desc=f\"Featurizing {desc}\"):\n",
    "        X[i] = extract_features(fen)\n",
    "    return X, y\n",
    "\n",
    "def load_split_save(train_csv: str,\n",
    "                    val_csv: str,\n",
    "                    train_size: int = 1_000_000,\n",
    "                    val_size: int = 100_000):\n",
    "\n",
    "    # Check for existing Parquet files\n",
    "    if os.path.exists(\"X_train.parquet\") and os.path.exists(\"y_train.parquet\") \\\n",
    "       and os.path.exists(\"X_val.parquet\") and os.path.exists(\"y_val.parquet\"):\n",
    "        print(\"Loading pre-saved Parquet datasets...\")\n",
    "        X_train = pd.read_parquet(\"X_train.parquet\").values\n",
    "        y_train = pd.read_parquet(\"y_train.parquet\")[\"value\"].values\n",
    "        X_val   = pd.read_parquet(\"X_val.parquet\").values\n",
    "        y_val   = pd.read_parquet(\"y_val.parquet\")[\"value\"].values\n",
    "        return X_train, y_train, X_val, y_val\n",
    "\n",
    "    df_train = pd.read_csv(train_csv)\n",
    "    df_val   = pd.read_csv(val_csv)\n",
    "\n",
    "    X_train, y_train = process_dataset(df_train, train_size, 'train')\n",
    "    X_val,   y_val   = process_dataset(df_val,   val_size,   'val')\n",
    "\n",
    "    pd.DataFrame(X_train).to_parquet(\"X_train.parquet\", index=False)\n",
    "    pd.DataFrame({\"value\": y_train}).to_parquet(\"y_train.parquet\", index=False)\n",
    "    pd.DataFrame(X_val).to_parquet(\"X_val.parquet\", index=False)\n",
    "    pd.DataFrame({\"value\": y_val}).to_parquet(\"y_val.parquet\", index=False)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def train_logistic_regression(X_train: np.ndarray, y_train: np.ndarray) -> LogisticRegression:\n",
    "    print(\"Fitting logistic regression...\")\n",
    "    lr = LogisticRegression(max_iter=1000, verbose=1)\n",
    "    lr.fit(X_train, y_train)\n",
    "    return lr\n",
    "\n",
    "\n",
    "X_train, y_train, X_val, y_val = load_split_save(\n",
    "    'train.csv', 'val.csv',\n",
    "    train_size=1_000_000, val_size=100_000\n",
    ")\n",
    "\n",
    "lr = train_logistic_regression(X_train, y_train)\n",
    "lr_acc = lr.score(X_val, y_val)\n",
    "print(f\"Logistic Regression - Validation accuracy: {lr_acc:.4f}\", end='')\n",
    "\n",
    "joblib.dump(lr, \"chess_lr.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "828d1ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.56666666,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6fb1832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35781898, 0.06956067, 0.57262036])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba(X_val[:1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d2e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3dc9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c02e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
