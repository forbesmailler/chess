"""NNUE training pipeline.

Reads binary .bin training data generated by the C++ self-play engine,
trains a small NNUE (773 -> 256 -> 32 -> 1) with ClippedReLU activations.
C++ inference uses tanh on the single output, scaled by MATE_VALUE.
Training returns tanh of the single output and uses MSE loss against a
blended target of search eval and game result.
"""

import argparse
import os
import struct
import sys
import warnings
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset, random_split

sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent))
from config.load_config import engine, training

warnings.filterwarnings("ignore", message="Cannot set number of intraop threads")
torch.set_num_threads(os.cpu_count())

_eng = engine()
_trn = training()

POSITION_SIZE = 42  # bytes per training position (binary struct layout)

INPUT_SIZE = _eng["nnue"]["input_size"]
HIDDEN1_SIZE = _eng["nnue"]["hidden1_size"]
HIDDEN2_SIZE = _eng["nnue"]["hidden2_size"]
OUTPUT_SIZE = _eng["nnue"]["output_size"]


class NNUE(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(INPUT_SIZE, HIDDEN1_SIZE)
        self.fc2 = nn.Linear(HIDDEN1_SIZE, HIDDEN2_SIZE)
        self.fc3 = nn.Linear(HIDDEN2_SIZE, OUTPUT_SIZE)
        self.crelu = nn.Hardtanh(0.0, 1.0)

    def forward(self, x):
        x = self.crelu(self.fc1(x))
        x = self.crelu(self.fc2(x))
        return torch.tanh(self.fc3(x)).squeeze(-1)  # range [-1, 1]


def extract_features(piece_placement, side_to_move, castling=0, en_passant_file=255):
    """Extract 773 features from binary position data.

    Features are from side-to-move perspective:
    - 0-383: own pieces (6 types x 64 squares)
    - 384-767: opponent pieces (6 types x 64 squares)
    - 768-771: castling (own KS, own QS, opp KS, opp QS)
    - 772: en passant available

    When black to move, the board is vertically flipped.
    """
    features = np.zeros(INPUT_SIZE, dtype=np.float32)
    white_to_move = side_to_move == 0

    # Vectorized nibble extraction: decode all 64 squares at once
    raw = np.frombuffer(piece_placement, dtype=np.uint8)
    high = (raw >> 4) & 0x0F  # even squares
    low = raw & 0x0F  # odd squares
    nibbles = np.empty(64, dtype=np.uint8)
    nibbles[0::2] = high
    nibbles[1::2] = low

    # Find non-empty squares
    occupied = np.nonzero(nibbles)[0]
    for sq in occupied:
        nibble = int(nibbles[sq])
        if nibble <= 6:
            piece_type, piece_color = nibble - 1, 0
        else:
            piece_type, piece_color = nibble - 7, 1

        is_own = (piece_color == 0) == white_to_move
        feature_sq = sq if white_to_move else (sq ^ 56)
        offset = 0 if is_own else 384
        features[offset + piece_type * 64 + feature_sq] = 1.0

    # Castling rights from STM perspective
    # Binary encoding: bit3=WK, bit2=WQ, bit1=BK, bit0=BQ
    if white_to_move:
        features[768] = float((castling >> 3) & 1)
        features[769] = float((castling >> 2) & 1)
        features[770] = float((castling >> 1) & 1)
        features[771] = float(castling & 1)
    else:
        features[768] = float((castling >> 1) & 1)
        features[769] = float(castling & 1)
        features[770] = float((castling >> 3) & 1)
        features[771] = float((castling >> 2) & 1)

    features[772] = float(en_passant_file != 255)

    return features


class SelfPlayDataset(Dataset):
    """Dataset reading binary training positions."""

    def __init__(self, filepath, eval_weight=0.75):
        self.filepath = filepath
        self.eval_weight = eval_weight

        file_size = Path(filepath).stat().st_size
        self.num_positions = file_size // POSITION_SIZE

        with open(filepath, "rb") as f:
            self.data = f.read()

    def __len__(self):
        return self.num_positions

    def __getitem__(self, idx):
        offset = idx * POSITION_SIZE
        raw = self.data[offset : offset + POSITION_SIZE]

        piece_placement = raw[0:32]
        side_to_move = raw[32]
        castling = raw[33]
        en_passant_file = raw[34]
        search_eval = struct.unpack("<f", raw[35:39])[0]
        game_result = raw[39]

        features = extract_features(
            piece_placement, side_to_move, castling, en_passant_file
        )

        # Eval target: clip search eval to [-1, 1]
        mate_value = float(_eng["mate_value"])
        eval_scalar = np.clip(search_eval / mate_value, -1.0, 1.0)

        # Result target: 0=loss -> -1, 1=draw -> 0, 2=win -> +1
        result_scalar = float(game_result - 1)

        # Blend eval and result targets
        target = (
            self.eval_weight * eval_scalar + (1.0 - self.eval_weight) * result_scalar
        )

        return torch.from_numpy(features), torch.tensor(target, dtype=torch.float32)


def train(args):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    num_cores = os.cpu_count()
    print(f"Using device: {device}, threads: {num_cores}")

    dataset = SelfPlayDataset(args.data, eval_weight=args.eval_weight)
    num_positions = len(dataset)
    print(f"Loaded {num_positions} positions from {args.data}")

    # Split into train/val
    val_size = max(1, int(len(dataset) * _trn["training"]["val_split"]))
    train_size = len(dataset) - val_size
    train_set, val_set = random_split(
        dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(_trn["training"]["random_seed"]),
    )

    use_cuda = device.type == "cuda"
    num_workers = (
        min(_trn["training"]["max_data_workers"], num_cores) if not use_cuda else 0
    )
    train_loader = DataLoader(
        train_set,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=use_cuda,
        persistent_workers=num_workers > 0,
    )
    val_loader = DataLoader(
        val_set,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=use_cuda,
        persistent_workers=num_workers > 0,
    )

    model = NNUE().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    scaler = torch.amp.GradScaler(enabled=use_cuda)

    best_val_loss = float("inf")
    patience_counter = 0
    best_state = None
    epoch_log = []

    for epoch in range(args.epochs):
        # Training
        model.train()
        train_loss = 0.0
        for features, targets in train_loader:
            features, targets = features.to(device), targets.to(device)

            with torch.amp.autocast(device_type=device.type, enabled=use_cuda):
                output = model(features)
                loss = F.mse_loss(output, targets)

            optimizer.zero_grad()
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            train_loss += loss.item() * features.size(0)

        train_loss /= train_size

        # Validation
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for features, targets in val_loader:
                features, targets = features.to(device), targets.to(device)
                with torch.amp.autocast(device_type=device.type, enabled=use_cuda):
                    output = model(features)
                    loss = F.mse_loss(output, targets)
                val_loss += loss.item() * features.size(0)

        val_loss /= val_size

        improved = val_loss < best_val_loss
        epoch_log.append((epoch + 1, train_loss, val_loss, improved))

        print(
            f"Epoch {epoch + 1}/{args.epochs} - "
            f"train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}"
        )

        # Early stopping
        if improved:
            best_val_loss = val_loss
            patience_counter = 0
            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}
            if args.output:
                torch.save(best_state, args.output)
                print(f"  Saved best model to {args.output}")
        else:
            patience_counter += 1
            if patience_counter >= args.patience:
                print(f"Early stopping at epoch {epoch + 1}")
                break

    print(f"Training complete. Best val loss: {best_val_loss:.4f}")
    if args.output:
        print(f"Model saved to {args.output}")

    if args.log:
        _write_log(args, num_positions, epoch_log, best_val_loss)

    return best_state


def _write_log(args, num_positions, epoch_log, best_val_loss):
    log_path = Path(args.log)
    log_path.parent.mkdir(parents=True, exist_ok=True)
    lines = [
        "# Training Log",
        "",
        f"- **Positions**: {num_positions:,}",
        f"- **Batch size**: {args.batch_size}",
        f"- **Learning rate**: {args.lr}",
        f"- **Eval weight**: {args.eval_weight}",
        f"- **Best val loss**: {best_val_loss:.6f}",
        f"- **Epochs run**: {len(epoch_log)}/{args.epochs}",
        "",
        "| Epoch | Train Loss | Val Loss | Best |",
        "|------:|-----------:|---------:|:----:|",
    ]
    for epoch, tl, vl, improved in epoch_log:
        marker = "*" if improved else ""
        lines.append(f"| {epoch} | {tl:.6f} | {vl:.6f} | {marker} |")
    lines.append("")
    log_path.write_text("\n".join(lines))
    print(f"Training log written to {log_path}")


def main():
    parser = argparse.ArgumentParser(description="Train NNUE from self-play data")
    parser.add_argument(
        "--data", required=True, help="Path to binary training data (.bin)"
    )
    parser.add_argument(
        "--output", default="nnue_weights.pt", help="Output PyTorch model path"
    )
    parser.add_argument("--epochs", type=int, default=_trn["training"]["epochs"])
    parser.add_argument(
        "--batch-size", type=int, default=_trn["training"]["batch_size"]
    )
    parser.add_argument("--lr", type=float, default=_trn["training"]["lr"])
    parser.add_argument("--patience", type=int, default=_trn["training"]["patience"])
    parser.add_argument(
        "--eval-weight",
        type=float,
        default=_trn["training"]["eval_weight"],
        help="Weight for search eval vs game result in target (0-1)",
    )
    parser.add_argument(
        "--log",
        help="Path to write training log as markdown (.md)",
    )
    args = parser.parse_args()

    train(args)


if __name__ == "__main__":
    main()
