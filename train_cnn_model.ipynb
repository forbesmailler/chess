{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.9806, Val Loss: 0.9586\n",
      "Epoch [2/50], Train Loss: 0.9539, Val Loss: 0.9465\n",
      "Epoch [3/50], Train Loss: 0.9470, Val Loss: 0.9458\n",
      "Epoch [4/50], Train Loss: 0.9434, Val Loss: 0.9425\n",
      "Epoch [5/50], Train Loss: 0.9415, Val Loss: 0.9434\n",
      "Epoch [6/50], Train Loss: 0.9395, Val Loss: 0.9392\n",
      "Epoch [7/50], Train Loss: 0.9381, Val Loss: 0.9404\n",
      "Epoch [8/50], Train Loss: 0.9361, Val Loss: 0.9396\n",
      "Epoch [9/50], Train Loss: 0.9350, Val Loss: 0.9373\n",
      "Epoch [10/50], Train Loss: 0.9336, Val Loss: 0.9412\n",
      "Epoch [11/50], Train Loss: 0.9326, Val Loss: 0.9386\n",
      "Epoch [12/50], Train Loss: 0.9313, Val Loss: 0.9346\n",
      "Epoch [13/50], Train Loss: 0.9304, Val Loss: 0.9349\n",
      "Epoch [14/50], Train Loss: 0.9295, Val Loss: 0.9350\n",
      "Epoch [15/50], Train Loss: 0.9288, Val Loss: 0.9342\n",
      "Epoch [16/50], Train Loss: 0.9280, Val Loss: 0.9348\n",
      "Epoch [17/50], Train Loss: 0.9272, Val Loss: 0.9391\n",
      "Epoch [18/50], Train Loss: 0.9264, Val Loss: 0.9331\n",
      "Epoch [19/50], Train Loss: 0.9258, Val Loss: 0.9325\n",
      "Epoch [20/50], Train Loss: 0.9248, Val Loss: 0.9342\n",
      "Epoch [21/50], Train Loss: 0.9242, Val Loss: 0.9364\n",
      "Epoch [22/50], Train Loss: 0.9237, Val Loss: 0.9320\n",
      "Epoch [23/50], Train Loss: 0.9230, Val Loss: 0.9333\n",
      "Epoch [24/50], Train Loss: 0.9227, Val Loss: 0.9336\n",
      "Epoch [25/50], Train Loss: 0.9221, Val Loss: 0.9312\n",
      "Epoch [26/50], Train Loss: 0.9216, Val Loss: 0.9345\n",
      "Epoch [27/50], Train Loss: 0.9214, Val Loss: 0.9335\n",
      "Epoch [28/50], Train Loss: 0.9206, Val Loss: 0.9348\n",
      "Epoch [29/50], Train Loss: 0.9203, Val Loss: 0.9319\n",
      "Epoch [30/50], Train Loss: 0.9197, Val Loss: 0.9320\n",
      "Early stopping triggered\n",
      "Final Test Loss: 0.9388\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "from chess_cnn import ChessCNN\n",
    "\n",
    "LEARNING_RATE = 1e-3           # You may experiment with values like 5e-3 for faster learning\n",
    "NUM_EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "BATCH_SIZE = 512                # Configurable batch size\n",
    "\n",
    "# --------------------------\n",
    "# Data Preparation and Splitting (with batching)\n",
    "# --------------------------\n",
    "\n",
    "df = pd.read_parquet(r\"C:\\Users\\forbe\\OneDrive\\Personal\\Documents\\repos\\chess_data\\df_features.parquet\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert features and labels from DataFrames.\n",
    "# X: first 780 columns from df; y: the \"result\" column.\n",
    "X = torch.tensor(df.iloc[:, :780].values, dtype=torch.float32).to(device)\n",
    "y = torch.tensor(df[\"result\"].values, dtype=torch.long).to(device)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(X, y)\n",
    "total_samples = len(dataset)\n",
    "\n",
    "# Split dataset: 80% training, 10% validation, 10% testing.\n",
    "train_size = int(0.8 * total_samples)\n",
    "val_size = int(0.1 * total_samples)\n",
    "test_size = total_samples - train_size - val_size\n",
    "\n",
    "# Use a seeded generator for reproducibility in the random split\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=generator)\n",
    "\n",
    "# Create DataLoaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --------------------------\n",
    "# Model, Weight Initialization, and Optimizer\n",
    "# --------------------------\n",
    "\n",
    "model = ChessCNN().to(device)\n",
    "\n",
    "# Weight initialization: Xavier uniform for Conv2d and Linear layers\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --------------------------\n",
    "# Training Loop with Early Stopping (with batching)\n",
    "# --------------------------\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    total_train = 0\n",
    "    \n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs_train = model(batch_x)\n",
    "        loss_train = criterion(outputs_train, batch_y)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss_train.item() * batch_x.size(0)\n",
    "        total_train += batch_x.size(0)\n",
    "        \n",
    "    train_loss = running_train_loss / total_train\n",
    "\n",
    "    # Evaluate on the validation set\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            outputs_val = model(batch_x)\n",
    "            loss_val = criterion(outputs_val, batch_y)\n",
    "            running_val_loss += loss_val.item() * batch_x.size(0)\n",
    "            total_val += batch_x.size(0)\n",
    "    val_loss = running_val_loss / total_val\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()  # Save best model's state\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "# Load the best model weights before testing\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# --------------------------\n",
    "# Testing (with batching)\n",
    "# --------------------------\n",
    "\n",
    "model.eval()\n",
    "running_test_loss = 0.0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        outputs_test = model(batch_x)\n",
    "        loss_test = criterion(outputs_test, batch_y)\n",
    "        running_test_loss += loss_test.item() * batch_x.size(0)\n",
    "        total_test += batch_x.size(0)\n",
    "        \n",
    "test_loss = running_test_loss / total_test\n",
    "print(f\"Final Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'pth/chess_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0765,  0.0813, -0.0924,  ...,  0.3564, -0.1260,  0.4368])\n"
     ]
    }
   ],
   "source": [
    "# Make sure the model is in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(X)                # Forward pass: get raw logits\n",
    "    probabilities = torch.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "    # Compute weighted score: probability of win (class 2) minus probability of loss (class 0)\n",
    "    weighted_score = probabilities[:, 2] - probabilities[:, 0]\n",
    "\n",
    "print(weighted_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0765,  0.0813, -0.0924,  ...,  0.3564, -0.1260,  0.4368])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
